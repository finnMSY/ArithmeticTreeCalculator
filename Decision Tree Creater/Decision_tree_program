{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8fe7e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2480 rows were removed from the dataset due to null values being found within them.\n",
      "Tree Built\n",
      "------\n",
      "5 (root)\n",
      "├── p (p)\n",
      "├── e (l)\n",
      "├── e (a)\n",
      "├── p (c)\n",
      "├── 19 (n)\n",
      "│   ├── 3 (w)\n",
      "│   │   ├── p (w)\n",
      "│   │   ├── e (n)\n",
      "│   │   └── e (c)\n",
      "│   ├── e (n)\n",
      "│   ├── p (r)\n",
      "│   └── e (k)\n",
      "└── p (f)\n",
      "------\n",
      "Here's a list of the predicted values: ['p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e']\n",
      "------\n",
      "Precision: 0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import anytree\n",
    "from anytree import NodeMixin, RenderTree\n",
    "\n",
    "data = pd.read_csv(r'~/Desktop/agaricus-lepiota.csv', header = None)\n",
    "data = data.values.tolist()\n",
    "#test_data = [[\"e\", \"easy\", \"short\", \"west\", \"none\"], [\"e\", \"some\", \"within\", \"west\", \"none\"], [\"p\", \"advanced\", \"within\", \"south\", \"flat\"], [\"p\", \"easy\", \"far\", \"north\", \"none\"], [\"e\", \"some\", \"short\", \"south\", \"wheel\"], [\"e\", \"advanced\", \"short\", \"south\", \"flat\"], [\"p\", \"easy\", \"within\", \"north\", \"wheel\"], [\"p\", \"some\", \"short\", \"west\", \"flat\"], [\"p\", \"advanced\", \"short\", \"west\", \"none\"], [\"p\", \"advanced\", \"far\", \"south\", \"wheel\"], [\"e\", \"advanced\", \"within\", \"north\", \"wheel\"]]\n",
    "#columnsList = [[\"Output\", \"Cap Shape\", \"Cap Surface\", \"Cap Color\", \"Bruises\", \"Odor\", \"Gill Attachment\", \"Gill Spacing\", \"Gill Size\", \"Gill color\", \"Stalk Shape\", \"Stalk Root\", \"Stalk Surface Above Ring\", \"Stalk Surface Below Surface\", \"Stalk Color Above Ring\", \"Stalk Color Below Ring\", \"Veil Type\", \"Veil Color\", \"Ring Number\", \"Ring Type\", \"Spore Print Color\", \"Population\", \"Habitat\"]]\n",
    "\n",
    "training_data = []\n",
    "testing_data = []\n",
    "testing_subset = round(len(data) - len(data)*0.1)\n",
    "for i in range(0, len(data)):\n",
    "    if i >= testing_subset:\n",
    "        testing_data.append(data[i])\n",
    "    else:\n",
    "        training_data.append(data[i])\n",
    "\n",
    "# This class makes up a singular not of a decision tree\n",
    "class WNode(NodeMixin):\n",
    "    def __init__(self, dataset, column=None, parent=None, weight=None):\n",
    "        super(WNode, self).__init__()\n",
    "        self.column = column # the column number this nodes splits on\n",
    "        self.parent = parent # the root node\n",
    "        self.weight = weight if parent is not None else None # Is the name of the branch connecting this node to the root node\n",
    "        self.data = dataset\n",
    "        self.childrenlist = []\n",
    "\n",
    "    def getChildren(self):\n",
    "        return self.childrenlist\n",
    "    \n",
    "    def add_child(self, node):\n",
    "        self.childrenlist.append(node)\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.data\n",
    "\n",
    "# this method finds the column to split on in a givent dataset and outputs that column number and the list of options within that column\n",
    "def findSplit(dataset):\n",
    "    # calculates the root entropy\n",
    "    totalTrue = 0 #Total data points which are edible\n",
    "    total = 0 #Total data points\n",
    "    columnValues = {}\n",
    "    totalCol = 0\n",
    "    for datalist in dataset:\n",
    "        total = total + 1\n",
    "        if datalist[0] == \"e\":\n",
    "            totalTrue = totalTrue + 1 \n",
    "        for i in range(1, len(datalist)):\n",
    "            key = \"true|{}|{}\".format(i, datalist[i])\n",
    "            keyTotal = \"total|{}|{}\".format(i, datalist[i])\n",
    "            if keyTotal in columnValues.keys():\n",
    "                columnValues[keyTotal] = columnValues.get(keyTotal) + 1\n",
    "            else:\n",
    "                columnValues[key] = 0\n",
    "                columnValues[keyTotal] = 1\n",
    "            if datalist[0] == \"e\":\n",
    "                columnValues[key] = columnValues.get(key) + 1\n",
    "            totalCol = i\n",
    "            \n",
    "    pTrue = totalTrue/total\n",
    "    pFalse = 1-(totalTrue/total)\n",
    "    if pTrue == 0 or pFalse == 0:\n",
    "        entropy_root = 0\n",
    "    else:\n",
    "        entropy_root = -pTrue*math.log(pTrue,2) - pFalse*math.log(pFalse,2)\n",
    "\n",
    "    #calculates the information gain of all the columns\n",
    "    trueNum = 0\n",
    "    totalNum = 0\n",
    "    pTrue = 0\n",
    "    pFalse = 0\n",
    "    info_gains = {}\n",
    "    for i in range(1, totalCol+1):\n",
    "        entropies = []\n",
    "        for key in columnValues.keys():\n",
    "            entropyList = []\n",
    "            if \"|{}|\".format(i) in key:\n",
    "                if \"true\" in key:\n",
    "                    trueNum = columnValues.get(key)\n",
    "                else:\n",
    "                    totalNum = columnValues.get(key)\n",
    "                    pTrue = trueNum/totalNum\n",
    "                    pFalse = 1-(trueNum/totalNum)\n",
    "                    if pTrue == 0 or pFalse == 0:\n",
    "                        entropy = 0\n",
    "                    else:\n",
    "                        entropy = -pTrue*math.log(pTrue,2) - pFalse*math.log(pFalse,2)\n",
    "                    entropyList.append(entropy)\n",
    "                    entropyList.append(totalNum)\n",
    "                    entropies.append(entropyList)\n",
    "\n",
    "        info_gain = entropy_root\n",
    "        for entropyList in entropies:\n",
    "            info_gain = info_gain - (entropyList[1] / total) * entropyList[0]\n",
    "        info_gains[i] = info_gain\n",
    "    \n",
    "    # finds the split\n",
    "    ig_set = set(info_gains.values())\n",
    "    if len(ig_set) > 1 or (list(ig_set)[0] != 0):\n",
    "        split = [k for k, v in info_gains.items() if v == max(list(info_gains.values()))]\n",
    "        if len(split) > 1:\n",
    "            split = [split[0]]\n",
    "            \n",
    "        split_options = set()        \n",
    "        for key in columnValues.keys():\n",
    "            if \"|{}|\".format(split[0]) in key:\n",
    "                x = key.split(\"|\")\n",
    "                split_options.add(x[len(x)-1])\n",
    "        \n",
    "        split.append(split_options)\n",
    "        split.append(max(list(info_gains.values())))\n",
    "    else:\n",
    "        split = findOutput(dataset)  \n",
    "    return split\n",
    "\n",
    "#this method builds the decision tree\n",
    "def buildTree(r, sd, current_depth):\n",
    "    if current_depth < stopping_depth:\n",
    "        current_depth += 1\n",
    "        if sd != \"p\" and sd != \"e\":\n",
    "            dList = r.getData()\n",
    "            splitCol = sd[0]\n",
    "            for value in sd[1]:\n",
    "                new_dataList = []\n",
    "                for datapoint in dList:\n",
    "                    if datapoint[splitCol] == value:\n",
    "                        rowList = []\n",
    "                        for i in range(0, len(datapoint)):\n",
    "                            if i != splitCol:\n",
    "                                rowList.append(datapoint[i])\n",
    "                        new_dataList.append(rowList)\n",
    "                new_split = findSplit(new_dataList)\n",
    "                node = WNode(new_dataList, column=new_split[0], parent=r, weight=value)\n",
    "                r.add_child(node)\n",
    "                buildTree(node, new_split, current_depth)   \n",
    "                \n",
    "#this methods returns the output of the leaf nodes\n",
    "def findOutput(data):\n",
    "    output = set()\n",
    "    for row in data:\n",
    "        output.add(row[0])\n",
    "    if len(output) == 1:\n",
    "        return list(output)[0]\n",
    "    \n",
    "#this method removes all the null values from a given dataset\n",
    "def removeNull(data):\n",
    "    counter = 0\n",
    "    temp = []\n",
    "    for row in data:\n",
    "        if row[11] != \"?\": \n",
    "            temp.append(row)\n",
    "            counter += 1\n",
    "    print(\"{} rows were removed from the dataset due to null values being found within them.\".format(len(data)-counter)) \n",
    "    return temp      \n",
    "\n",
    "#this method predicts an output of a given data list\n",
    "def predict(d, node):\n",
    "    if len(node.getChildren()) == 0:\n",
    "        try:\n",
    "            node.column.isdigit()\n",
    "            predicted_values.append(node.column)\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        split = node.column\n",
    "        for child in node.getChildren():\n",
    "            if d[split-1] == child.weight:\n",
    "                d.pop(split)\n",
    "                predict(d, child)\n",
    "    \n",
    "cleansed_data = removeNull(data)\n",
    "training_data = [] #the dataset which the decision tree will be trained on\n",
    "testing_data = [] #the dataset which will be tested on the decision tree\n",
    "testing_data_outputs = []\n",
    "stopping_depth = 3\n",
    "current_depth = 0\n",
    "testing_subset = round(len(cleansed_data) - len(cleansed_data)*0.1)\n",
    "for i in range(0, len(cleansed_data)):\n",
    "    if i >= testing_subset:\n",
    "        testing_data.append(cleansed_data[i])\n",
    "    else:\n",
    "        training_data.append(cleansed_data[i])\n",
    "\n",
    "# testing_data = pd.read_csv(r'~/Desktop/testing_data.csv', header = None)\n",
    "# testing_data = testing_data.values.tolist()\n",
    "\n",
    "for d in testing_data:\n",
    "    output = d.pop(0)\n",
    "    testing_data_outputs.append(output)\n",
    "\n",
    "splitdata = findSplit(training_data)\n",
    "root = WNode(training_data, column=splitdata[0])\n",
    "buildTree(root, splitdata, current_depth)\n",
    "print(\"Tree Built\")\n",
    "print(\"------\")\n",
    "\n",
    "#displays the decision tree\n",
    "for pre, _, node in RenderTree(root):\n",
    "    if len(node.getChildren()) > 0:\n",
    "        print(\"%s%s (%s)\" % (pre, node.column, node.weight or \"root\"))\n",
    "    else:\n",
    "        print(\"%s%s (%s)\" % (pre, findOutput(node.getData()), node.weight or 0))\n",
    "                \n",
    "predicted_values = []\n",
    "for row in testing_data:\n",
    "    predict(row, root)\n",
    "\n",
    "#evaluates the decision tree and calculates the precision\n",
    "TP = 0\n",
    "FP = 0\n",
    "for i in range(0, len(predicted_values)):\n",
    "    if testing_data_outputs[i] == \"e\" and predicted_values[i] == \"e\":\n",
    "        TP += 1\n",
    "    elif testing_data_outputs[i] == \"p\" and predicted_values[i] == \"e\":\n",
    "        FP += 1\n",
    "        \n",
    "print(\"------\")\n",
    "print(\"Here's a list of the predicted values: {}\".format(predicted_values))\n",
    "if TP == 0 and FP == 0:\n",
    "    precision = 0\n",
    "else:\n",
    "    precision = TP / (TP + FP)\n",
    "print(\"------\")\n",
    "print(\"Precision: {}\".format(precision))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5a827",
   "metadata": {},
   "source": [
    "# Code Explanation\n",
    "\n",
    "### Splitting the data\n",
    "The program finds the column in which to split the data on by first find the amount of values which are true(edible) and false (poisonous) for each possible option of each column. This data is then put in a dictionary as the program loops through every option of every column. With this data we can calculate the entropy for each option in each column and therefore find the information gain of each column. These values get put into a list so we can find the maximum value. Once we find the column number which represents this value we return this column number. If a more than one column have the same max information gain I selected the lowest column number as the split. \n",
    "\n",
    "### Predicts the output of a given data list\n",
    "This works by starting at the root node, seeing what column it's split on, then comparing the value in that column in the test data to all the options from that column. Once there is a match the test data will go down that path repeat this process on the node connected to the path. Once this is finished, it takes the test data down the tree to a leaf noe with no children nodes. The program then takes the dataset designated to taht node and check if all the outputs are the same value (they should be) then if they are, take that value and print it to the user. This is the output from that test data and the prediciton the classifier has made as to whether this mushroom is edible or poisonous.\n",
    "\n",
    "### Stopping Depths\n",
    "At a stopping_depth of 2, we have the following tree:\n",
    "```\n",
    "5 (root)\n",
    "├── p (c)\n",
    "├── p (f)\n",
    "├── e (l)\n",
    "├── p (p)\n",
    "├── e (a)\n",
    "└── 19 (n)\n",
    "    ├── e (k)\n",
    "    ├── None (w)\n",
    "    ├── p (r)\n",
    "    └── e (n)\n",
    "```\n",
    "In this tree we have one split (excluding the root) on column 19 on the path 'n' which was 4 chilren.\n",
    "At a stopping_depth of 3, we have the following tree:\n",
    "```\n",
    "5 (root)\n",
    "├── p (c)\n",
    "├── p (f)\n",
    "├── e (l)\n",
    "├── p (p)\n",
    "├── e (a)\n",
    "└── 19 (n)\n",
    "    ├── e (k)\n",
    "    ├── 3 (w)\n",
    "    │   ├── e (c)\n",
    "    │   ├── p (w)\n",
    "    │   └── e (n)\n",
    "    ├── p (r)\n",
    "    └── e (n)\n",
    " ```\n",
    "In this tree we have two splits (excluding the root) on column 19 on the path 'n' which has 4 chilren, and on column 3 on path 'w' within the split on column 19, this as 3 children. Since all of these nodes are leaf nodes this tree is complete.\n",
    "At a stopping_depth of 3, we have the following tree:\n",
    "```\n",
    "5 (root)\n",
    "├── p (c)\n",
    "├── p (f)\n",
    "├── e (l)\n",
    "├── p (p)\n",
    "├── e (a)\n",
    "└── 19 (n)\n",
    "    ├── e (k)\n",
    "    ├── 3 (w)\n",
    "    │   ├── e (c)\n",
    "    │   ├── p (w)\n",
    "    │   └── e (n)\n",
    "    ├── p (r)\n",
    "    └── e (n)\n",
    "```\n",
    "This tree is the same tree as before, which proves that the tree at a stopping_depth of 3 was the full and complete tree.\n",
    "\n",
    "### The training and test datasets\n",
    "In order to test real data on this decision tree, I took the bottom 10% of the dataset and put it in the test_dataset and the top 90% into the training_dataset. This leaves us with plent of data for training the tree and a decent amount to test on the tree as well. \n",
    "There is the option to input your own test data from a new csv file by uncommenting the lines\n",
    "```\n",
    "#testing_data = pd.read_csv(r'~/Desktop/testing_data.csv', header = None)\n",
    "#testing_data = data.values.tolist()\n",
    "```\n",
    "and replacing '~/Desktop/testing_data.csv' with your own file and location.\n",
    "\n",
    "### The evaluation\n",
    "I evaluated the data using the 'precision' performance measure as it seems to focus on False Positives. I feel like with the topic of this dataset its important to have as little false positives as possible as a poisonous mushroom being classed as edible can be dangerous. Precision is used to find the ratio of actual edible mushrooms in all mushrooms which were classified as edible. The higher this performance measure is the better and safer this classifer will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a081104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
