{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d15603",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# <u>Naive Bayes Classifier Report </u>\n",
    "**Explain and motivate the chosen representation & data preprocessing:** <br>\n",
    "I have chosen my current model due to the improvements it has over standard Naive Bayes implementation, which gives it a much better accuracy and successful prediction rate. The methodology of my program consists of the standard Naive Bayes classifier (further information given later on in this report) along with various improvements such as n-grams, smoothing, pre-processing, the inclusion of word frequency, and working in the log-space (further information given later on in this report).\n",
    "For data preprocessing, I split up the training dataset into a mock test dataset which contained the last 10% of the data, and the training dataset, which contained the first 90%. This was done so I could test my program before submitting it to Kaggle and obtaining my training accuracy score. The main act of preprocessing I practiced was removing a group of words that had an extremely high frequency from the text. I determined these words by measuring every word’s frequency within the total text of each row and then removed all the words with a frequency greater than half the frequency of the most common word. In the training dataset this ended up being the top 19 most common words.  \n",
    "\n",
    "**Explain the idea behind the model improvements and their implementation:** <br>\n",
    "The various improvements I made to the base version of my Naive Bayes classifier were adding n-grams up to three words (combining up to three words into one if every instance of those two words next two each other has the same output class), Laplace smoothing, including the frequency of each word in my calculations, and my overall calculation of the probability of each word occurring.\n",
    "1. While the n-gram feature I’ve created does produce the same Kaggle and Training accuracy as the classifier without engrams, it might prove more effective on a larger dataset. However, it does take a fairly long time to train my classifier in this model, which definitely makes it less accessible compared to the model without n-grams.\n",
    "2. I’ve implemented Laplace smoothing into my model, as it makes probabilities of words that don’t exist in my training dataset equal to 0.5 or 1/number of options (1, 0), which is equal to 0.5.\n",
    "3. Rather than just combining all the probabilities of each word occurring, I found the sum of the probabilities of each word occurring multiplied by the frequency of the word. This balances the weights of all the words as if a word is more frequent, it will more likely occur multiple times within the text of the test data, which isn’t accounted for when you just use the probability of it being in the text. \n",
    "4. Firstly, I conducted all my multiplications in the log space as multiplying tiny numbers can give computational issues, which I want to avoid. I also left out the denominator of the final probability calculations for each class as if it is the same for every class, then it wouldn’t affect the sizes of the probabilities in relation to each other class, so it can be ignored. I’ve also changed the overall final equation to be “probability of it being a certain class - the probability that it isn’t that class” rather than “probability of it being a certain class + the probability that it is that class”. You could also say that I’m finding the probabilities that each class isn’t the correct one and then picking the class with the lowest fail chance. This technique was mentioned in the pdf “Tackling the Poor Assumptions of Naive Bayes Text Classifiers\". Written by Rennie at al. (2003), which was provided in Canvas. \n",
    "\n",
    "**Explain the evaluation procedure:** <br>\n",
    "The evaluation of my data can be split into two values, the Kaggle accuracy and the training accuracy. The Kaggle accuracy is calculated by Kaggle when I submit my list of predictions to the website. The training accuracy is determined by testing the classifier on the smaller percentage of the training data I’ve classed as the test dataset. In this model, I’ve removed the bottom 10% from the training dataset and put it in a new dataset which I then tested my classifier on. I then separated the output classes from the abstract text and put them in a separate list, which I then compared to the predicted class outputs to find the training accuracy. This was done so I could test my program before submitting it to Kaggle and obtaining my training accuracy score.\n",
    "\n",
    "**Include and explain the training/validation results for the standard and improved Naive Bayes model:** <br>\n",
    "The base Naive Bayes implementation: Training accuracy: 0.675, Kaggle accuracy: 0.72 <br>\n",
    "These are the baseline results for the standard Naive Bayes implementation. Since I don’t have access to Mitchell's \"Machine Learning\" textbook, I just used the classic equation to calculate my predictions P(B|A)=P(A|B)P(B)/P(A). P(A|B) includes the probability of every word in the abstract text of occurring and the probability of every word not in the abstract text from not occurring. All these probabilities, as well as the probability of the class from occurring, are multiplied together and then divided by the probability of each word occurring regarding of class multiplied together. This gives you the probability of a certain class being the correct class. You then find the maximum probability to find the designated class prediction. \n",
    "\n",
    "The improved Naive Bayes implementation: Training accuracy: 0.965, Kaggle accuracy: 0.97 <br>\n",
    "This probability is a huge improvement of the standard implementation, which is due to all the pre-processing and improvements made as specified previously in this report.\n",
    "\n",
    "Other models include: <br> \n",
    "The improved Naive Bayes implementation - pre-processing and smoothing: Training accuracy: 0.9225, Kaggle accuracy: 0.94333 <br>\n",
    "This probability, while not being the best, is a significant improvement from the standard implementation, which is due to most of the improvements specified previously in this report. This is also a major checkpoint/milestone within this project, as it was the first model to give me a significant increase in accuracy from my standard implementation.\n",
    "\n",
    "The improved Naive Bayes implementation + n-grams: Training accuracy: 0.965, Kaggle accuracy: 0.97 <br>\n",
    "The model, while having the same values, is the other improved model, includes n-grams up to three words. While it does take a much longer time to find all the predictions due to the n-gram calculations, I feel that it might produce better accuracy over a much larger dataset. <br>\n",
    "*For a full list of the predictions, just run the corresponding code for the desired model and uncomment the specified print value to obtain all the predictions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c84a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.675\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Base Naive Bayes implementation - Kaggle accuracy: 0.71, Training accuracy: 0.675\n",
    "\n",
    "import pandas as pd\n",
    "import math as m\n",
    "df = pd.read_csv(r'~/Onedrive/Desktop/trg.csv')\n",
    "training_data = df.values.tolist()\n",
    "df = pd.read_csv(r'~/Onedrive/Desktop/tst.csv')\n",
    "# Uncomment this line when checking kaggle accuracy\n",
    "#test_data = df.values.tolist()\n",
    "\n",
    "# Comment out these lines when checking kaggle accuracy\n",
    "test_data = []\n",
    "test_values = []\n",
    "temp_test_data = []\n",
    "for i in range(len(training_data)):\n",
    "    if i >= (len(training_data) * 0.9):\n",
    "        temp = [training_data[i][0], training_data[i][2]]\n",
    "        test_values.append(training_data[i][1])\n",
    "        test_data.append(temp)\n",
    "    else:\n",
    "        temp_test_data.append(training_data[i])\n",
    "training_data = temp_test_data\n",
    "# End of lines to be commented out\n",
    "\n",
    "totalRows = 0\n",
    "totalWords = set()\n",
    "wordCounts = {}\n",
    "totalWordCounts = {}\n",
    "classCounts = {}\n",
    "wordProbs = {}\n",
    "predictions = []\n",
    "\n",
    "# Training the classifier\n",
    "def trainClassifier(data, totalRows):\n",
    "    for row in training_data:\n",
    "        totalRows += 1\n",
    "        classKey = \"{0}\".format(row[1])\n",
    "        if classKey in classCounts:\n",
    "            classCounts[classKey] = classCounts.get(classKey) + 1\n",
    "        else:\n",
    "            classCounts[classKey] = 1\n",
    "        \n",
    "        wordList = list(set(row[2].split(\" \")))\n",
    "        for word in wordList:\n",
    "            totalWords.add(word)\n",
    "            key = \"{0}|{1}\".format(word, row[1])\n",
    "            if key in wordCounts:\n",
    "                wordCounts[key] = wordCounts.get(key) + 1\n",
    "            else:\n",
    "                wordCounts[key] = 1\n",
    "\n",
    "            key2 = \"{0}\".format(word)\n",
    "            if key2 in totalWordCounts:\n",
    "                totalWordCounts[key2] = totalWordCounts.get(key2) + 1\n",
    "            else:\n",
    "                totalWordCounts[key2] = 1\n",
    "\n",
    "    for word in totalWordCounts:\n",
    "        for classifier in classCounts:\n",
    "            key_positive = \"P({0}=1|class={1})\".format(word, classifier)\n",
    "            key_negative = \"P({0}=0|class={1})\".format(word, classifier)\n",
    "            key = \"{0}|{1}\".format(word, classifier)\n",
    "            \n",
    "            if key in wordCounts:\n",
    "                numerator = wordCounts.get(key)\n",
    "                denominator = classCounts.get(classifier) \n",
    "                probability_positive = numerator / denominator\n",
    "                numerator = classCounts.get(classifier) - wordCounts.get(key) \n",
    "                probability_negative = numerator / denominator\n",
    "            else:\n",
    "                probability_positive = 0\n",
    "                probability_negative = 1\n",
    "            \n",
    "            wordProbs[key_positive] = probability_positive\n",
    "            wordProbs[key_negative] = probability_negative\n",
    "        \n",
    "    return totalRows\n",
    "        \n",
    "# Predicting the output classes for test_data\n",
    "def predict(data):\n",
    "    counter = 0\n",
    "    for row in data:\n",
    "        counter += 1\n",
    "        classifierScores = {}\n",
    "        for classifier in classCounts:\n",
    "            numerator = classCounts.get(classifier) / totalRows\n",
    "            demoninator = 1\n",
    "            wordList = row[1].split(\" \")\n",
    "            wordSet = set(wordList)\n",
    "            wordList = list(wordSet)\n",
    "            \n",
    "            for word in totalWords:\n",
    "                if word in wordList:\n",
    "                    key = \"P({0}=1|class={1})\".format(word, classifier) \n",
    "                    demoninator = demoninator * (totalWordCounts.get(word) / totalRows)\n",
    "                else:\n",
    "                    key = \"P({0}=0|class={1})\".format(word, classifier)\n",
    "                    demoninator = demoninator * ((totalRows - totalWordCounts.get(word)) / totalRows)\n",
    "                    \n",
    "                if key not in wordProbs:\n",
    "                    wordProbs[key] = 0 \n",
    "                numerator = numerator * wordProbs.get(key)\n",
    "            \n",
    "            finalProb = 0\n",
    "            if demoninator != 0:\n",
    "                finalProb = numerator / demoninator\n",
    "            classifierScores[finalProb] = classifier\n",
    "             \n",
    "        prediction = classifierScores.get(max(list(classifierScores.keys())))\n",
    "        temp_list = [counter, prediction]\n",
    "        predictions.append(temp_list)\n",
    "            \n",
    "totalRows = trainClassifier(training_data, totalRows)\n",
    "predict(test_data)\n",
    "\n",
    "# Uncomment to see total list of predictions\n",
    "#print(predictions)\n",
    "\n",
    "# Comment out these lines when checking kaggle accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(predictions)):\n",
    "    total += 1\n",
    "    if predictions[i][1] == test_values[i]:\n",
    "        correct += 1\n",
    "print(\"Accuracy: {0}\".format(correct/total))\n",
    "\n",
    "# Uncomment these two lines to generate a csv of the predictions to check kaggle accuracy\n",
    "#df = pd.DataFrame(predictions, columns=[\"id\", \"class\"])\n",
    "#df.to_csv('output.csv', index=False)\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a257445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9225\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Improved Naive Bayes implementation with calculations done in the log-sapce, and taking into account frequency of each word \n",
    "# in the abstract - Kaggle accuracy: 0.94333, Training accuracy: 0.9225\n",
    "\n",
    "import pandas as pd\n",
    "import math as m\n",
    "df = pd.read_csv(r'~/Onedrive/Desktop/trg.csv')\n",
    "training_data = df.values.tolist()\n",
    "df = pd.read_csv(r'~/Onedrive/Desktop/tst.csv')\n",
    "# Uncomment this line when checking kaggle accuracy\n",
    "#test_data = df.values.tolist()\n",
    "\n",
    "# Comment out these lines when checking kaggle accuracy\n",
    "test_data = []\n",
    "test_values = []\n",
    "temp_test_data = []\n",
    "for i in range(len(training_data)):\n",
    "    if i >= (len(training_data) * 0.9):\n",
    "        temp = [training_data[i][0], training_data[i][2]]\n",
    "        test_values.append(training_data[i][1])\n",
    "        test_data.append(temp)\n",
    "    else:\n",
    "        temp_test_data.append(training_data[i])\n",
    "training_data = temp_test_data\n",
    "# End of lines to be commented out\n",
    "\n",
    "totalRows = 0\n",
    "totalWords = set()\n",
    "wordCounts = {}\n",
    "totalWordCounts = {}\n",
    "classCounts = {}\n",
    "wordProbs = {}\n",
    "predictions = []\n",
    "\n",
    "# Training the classifier\n",
    "def trainClassifier(data, totalRows):\n",
    "    for row in training_data:\n",
    "        totalRows += 1\n",
    "        classKey = \"{0}\".format(row[1])\n",
    "        if classKey in classCounts:\n",
    "            classCounts[classKey] = classCounts.get(classKey) + 1\n",
    "        else:\n",
    "            classCounts[classKey] = 1\n",
    "        \n",
    "        wordList = list(set(row[2].split(\" \")))\n",
    "        for word in wordList:\n",
    "            totalWords.add(word)\n",
    "            key = \"{0}|{1}\".format(word, row[1])\n",
    "            if key in wordCounts:\n",
    "                wordCounts[key] = wordCounts.get(key) + 1\n",
    "            else:\n",
    "                wordCounts[key] = 1\n",
    "\n",
    "            key2 = \"{0}\".format(word)\n",
    "            if key2 in totalWordCounts:\n",
    "                totalWordCounts[key2] = totalWordCounts.get(key2) + 1\n",
    "            else:\n",
    "                totalWordCounts[key2] = 1\n",
    "\n",
    "    for word in totalWordCounts:\n",
    "        for classifier in classCounts:\n",
    "            key_negative = \"P({0}=0|class={1})\".format(word, classifier)\n",
    "            key = \"{0}|{1}\".format(word, classifier)\n",
    "            \n",
    "            if key in wordCounts:\n",
    "                numerator = totalWordCounts.get(word) - wordCounts.get(key)\n",
    "                denominator = totalRows - classCounts.get(classifier) \n",
    "                probability_negative = numerator / denominator\n",
    "            else:\n",
    "                probability_negative = 1\n",
    "            \n",
    "            wordProbs[key_negative] = probability_negative\n",
    "        \n",
    "    return totalRows\n",
    "        \n",
    "# Predicting the output classes for test_data\n",
    "def predict(data):\n",
    "    counter = 0\n",
    "    for row in data:\n",
    "        counter += 1\n",
    "        text = row[1]\n",
    "        wordList = text.split(\" \")\n",
    "        word_freq = {}\n",
    "        for word in wordList:\n",
    "            if word in word_freq:\n",
    "                word_freq[word] = word_freq.get(word) + 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "        \n",
    "        probs_of_class = {}\n",
    "        for classifier in classCounts:\n",
    "            prob = classCounts.get(classifier) / totalRows\n",
    "            if prob != 0:\n",
    "                total_class_prob = m.log(prob)\n",
    "            \n",
    "            sum_of_probs = 0\n",
    "            unique_wordList = list(set(wordList))\n",
    "            for word in totalWords:\n",
    "                key = \"P({0}=0|class={1})\".format(word, classifier)\n",
    "                \n",
    "                prob = wordProbs.get(key)\n",
    "                if word in word_freq and prob != 0:\n",
    "                    sum_of_probs += m.log(prob) * word_freq.get(word)\n",
    "            \n",
    "            class_prob = total_class_prob - sum_of_probs\n",
    "            probs_of_class[class_prob] = classifier\n",
    "        \n",
    "        prediction = probs_of_class.get(max(list(probs_of_class)))\n",
    "        temp_list = [counter, prediction]\n",
    "        predictions.append(temp_list)\n",
    "        \n",
    "totalRows = trainClassifier(training_data, totalRows)\n",
    "predict(test_data)\n",
    "\n",
    "# Uncomment to see total list of predictions\n",
    "#print(predictions)\n",
    "\n",
    "# Comment out these lines when checking kaggle accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(predictions)):\n",
    "    total += 1\n",
    "    #print(\"{0} = {1}\".format(predictions[i][1], test_values[i]))\n",
    "    if predictions[i][1] == test_values[i]:\n",
    "        correct += 1\n",
    "print(\"Accuracy: {0}\".format(correct/total))\n",
    "\n",
    "# Uncomment these two lines to generate a csv of the predictions to check kaggle accuracy\n",
    "#df = pd.DataFrame(predictions, columns=[\"id\", \"class\"])\n",
    "#df.to_csv('output.csv', index=False)\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbb2c041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.965\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Added lapace smoothing and removes all uncorrelated words (words with a very high frequency) \n",
    "# - Kaggle accuracy: 0.97, Training accuracy: 0.965\n",
    "\n",
    "import pandas as pd\n",
    "import math as m\n",
    "df = pd.read_csv(r'~/Onedrive/Desktop/trg.csv')\n",
    "training_data = df.values.tolist()\n",
    "df = pd.read_csv(r'~/Onedrive/Desktop/tst.csv')\n",
    "# Uncomment this line when checking kaggle accuracy\n",
    "#test_data = df.values.tolist()\n",
    "\n",
    "# Comment out these lines when checking kaggle accuracy\n",
    "test_data = []\n",
    "test_values = []\n",
    "temp_test_data = []\n",
    "for i in range(len(training_data)):\n",
    "    if i >= (len(training_data) * 0.9):\n",
    "        temp = [training_data[i][0], training_data[i][2]]\n",
    "        test_values.append(training_data[i][1])\n",
    "        test_data.append(temp)\n",
    "    else:\n",
    "        temp_test_data.append(training_data[i])\n",
    "training_data = temp_test_data\n",
    "# End of lines to be commented out\n",
    "\n",
    "totalRows = 0\n",
    "totalWords = set()\n",
    "wordCounts = {}\n",
    "totalWordCounts = {}\n",
    "classCounts = {}\n",
    "wordProbs = {}\n",
    "predictions = []\n",
    "\n",
    "# Training the classifier\n",
    "def trainClassifier(data, totalRows):\n",
    "    for row in training_data:\n",
    "        totalRows += 1\n",
    "        classKey = \"{0}\".format(row[1])\n",
    "        if classKey in classCounts:\n",
    "            classCounts[classKey] = classCounts.get(classKey) + 1\n",
    "        else:\n",
    "            classCounts[classKey] = 1\n",
    "        \n",
    "        wordList = list(set(row[2].split(\" \")))\n",
    "        for word in wordList:\n",
    "            totalWords.add(word)\n",
    "            key = \"{0}|{1}\".format(word, row[1])\n",
    "            if key in wordCounts:\n",
    "                wordCounts[key] = wordCounts.get(key) + 1\n",
    "            else:\n",
    "                wordCounts[key] = 1\n",
    "\n",
    "            key2 = \"{0}\".format(word)\n",
    "            if key2 in totalWordCounts:\n",
    "                totalWordCounts[key2] = totalWordCounts.get(key2) + 1\n",
    "            else:\n",
    "                totalWordCounts[key2] = 1\n",
    "    \n",
    "    # Removing words with high frequency\n",
    "    temp_list = list(totalWordCounts.items())\n",
    "    temp_list.sort(key = lambda x: x[1], reverse=True)\n",
    "    highest = temp_list[0][1]\n",
    "    for word in temp_list:\n",
    "        if word[1] < highest/2:\n",
    "            break\n",
    "        totalWords.remove(word[0])\n",
    "        totalWordCounts.pop(word[0])\n",
    "                \n",
    "    for word in totalWordCounts:\n",
    "        for classifier in classCounts:\n",
    "            key_negative = \"P({0}=0|class={1})\".format(word, classifier)\n",
    "            key = \"{0}|{1}\".format(word, classifier)\n",
    "            \n",
    "            # Probabilty calculations\n",
    "            if key in wordCounts:\n",
    "                numerator = totalWordCounts.get(word) - wordCounts.get(key) + 1\n",
    "                denominator = totalRows - classCounts.get(classifier) + 2 \n",
    "                probability_negative = numerator / denominator\n",
    "            else:\n",
    "                probability_negative = 0.5\n",
    "            \n",
    "            wordProbs[key_negative] = probability_negative\n",
    "        \n",
    "    return totalRows\n",
    "        \n",
    "# Predicting outputting classes\n",
    "def predict(data):\n",
    "    counter = 0\n",
    "    for row in data:\n",
    "        counter += 1\n",
    "        text = row[1]\n",
    "        wordList = text.split(\" \")\n",
    "        word_freq = {}\n",
    "        for word in wordList:\n",
    "            if word in word_freq:\n",
    "                word_freq[word] = word_freq.get(word) + 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "        \n",
    "        probs_of_class = {}\n",
    "        for classifier in classCounts:\n",
    "            prob = classCounts.get(classifier) / totalRows\n",
    "            if prob != 0:\n",
    "                total_class_prob = m.log(prob)\n",
    "            \n",
    "            unique_wordList = list(set(wordList))\n",
    "            sum_of_probs2 = 0\n",
    "            for word in unique_wordList:\n",
    "                key = \"P({0}=0|class={1})\".format(word, classifier)\n",
    "                prob = wordProbs.get(key)\n",
    "                if word in word_freq and prob != 0 and key in wordProbs:\n",
    "                    sum_of_probs2 += m.log(prob) * word_freq.get(word)\n",
    "            \n",
    "            class_prob = total_class_prob - sum_of_probs2\n",
    "            probs_of_class[class_prob] = classifier\n",
    "        \n",
    "        prediction = probs_of_class.get(max(list(probs_of_class)))\n",
    "        temp_list = [counter, prediction]\n",
    "        predictions.append(temp_list)\n",
    "    \n",
    "totalRows = trainClassifier(training_data, totalRows)\n",
    "predict(test_data)\n",
    "\n",
    "# Uncomment to see total list of predictions\n",
    "#print(predictions)\n",
    "\n",
    "# Comment out these lines when checking kaggle accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(predictions)):\n",
    "    total += 1\n",
    "    #print(\"{0} = {1}\".format(predictions[i][1], test_values[i]))\n",
    "    if predictions[i][1] == test_values[i]:\n",
    "        correct += 1\n",
    "print(\"Accuracy: {0}\".format(correct/total))\n",
    "\n",
    "# Uncomment these two lines to generate a csv of the predictions to check kaggle accuracy\n",
    "#df = pd.DataFrame(predictions, columns=[\"id\", \"class\"])\n",
    "#df.to_csv('output.csv', index=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75b4f50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.965\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Added n-gram in word groups of 2 - Kaggle accuracy: 0.97, Training accuracy:  0.965\n",
    "\n",
    "import pandas as pd\n",
    "import math as m\n",
    "df = pd.read_csv(r'~/Onedrive/Desktop/trg.csv')\n",
    "training_data = df.values.tolist()\n",
    "df = pd.read_csv(r'~/Onedrive/Desktop/tst.csv')\n",
    "# Uncomment this line when checking kaggle accuracy\n",
    "#test_data = df.values.tolist()\n",
    "\n",
    "# Comment out these lines when checking kaggle accuracy\n",
    "test_data = []\n",
    "test_values = []\n",
    "temp_test_data = []\n",
    "for i in range(len(training_data)):\n",
    "    if i >= (len(training_data) * 0.9):\n",
    "        temp = [training_data[i][0], training_data[i][2]]\n",
    "        test_values.append(training_data[i][1])\n",
    "        test_data.append(temp)\n",
    "    else:\n",
    "        temp_test_data.append(training_data[i])\n",
    "training_data = temp_test_data\n",
    "# End of lines to be commented out\n",
    "\n",
    "totalRows = 0\n",
    "totalWords = set()\n",
    "wordCounts = {}\n",
    "totalWordCounts = {}\n",
    "classCounts = {}\n",
    "wordProbs = {}\n",
    "predictions = []\n",
    "\n",
    "# Training classifier\n",
    "def trainClassifier(data, totalRows):\n",
    "    wordPairs = {}\n",
    "    for row in training_data:\n",
    "        totalRows += 1\n",
    "        classKey = \"{0}\".format(row[1])\n",
    "        if classKey in classCounts:\n",
    "            classCounts[classKey] = classCounts.get(classKey) + 1\n",
    "        else:\n",
    "            classCounts[classKey] = 1\n",
    "        \n",
    "        wordList = list(set(row[2].split(\" \")))\n",
    "        for word in wordList:\n",
    "            totalWords.add(word)\n",
    "            key = \"{0}|{1}\".format(word, row[1])\n",
    "            if key in wordCounts:\n",
    "                wordCounts[key] = wordCounts.get(key) + 1\n",
    "            else:\n",
    "                wordCounts[key] = 1\n",
    "\n",
    "            key2 = \"{0}\".format(word)\n",
    "            if key2 in totalWordCounts:\n",
    "                totalWordCounts[key2] = totalWordCounts.get(key2) + 1\n",
    "            else:\n",
    "                totalWordCounts[key2] = 1\n",
    "\n",
    "    # Removing words with high frequency\n",
    "    temp_list = list(totalWordCounts.items())\n",
    "    temp_list.sort(key = lambda x: x[1], reverse=True)\n",
    "    highest = temp_list[0][1]\n",
    "    for word in temp_list:\n",
    "        if word[1] < highest/2:\n",
    "            break\n",
    "        totalWords.remove(word[0])\n",
    "        totalWordCounts.pop(word[0])\n",
    "    \n",
    "    # Finding all pairs of words\n",
    "    for row in training_data:\n",
    "        wordList = row[2].split(\" \")\n",
    "        classifier = row[1]\n",
    "        for i in range(len(wordList)):\n",
    "            if i < len(wordList)-1:\n",
    "                key = \"{0}|{1}\".format(wordList[i], wordList[i+1])\n",
    "                if key in wordPairs:\n",
    "                    if (wordPairs.get(key)[1] != classifier):\n",
    "                        wordPairs.pop(key)\n",
    "                    else:\n",
    "                        classifier_and_count = [wordPairs.get(key)[0]+1, classifier]\n",
    "                        wordPairs[key] = classifier_and_count\n",
    "                else:\n",
    "                    classifier_and_count = [1, classifier]\n",
    "                    wordPairs[key] = classifier_and_count\n",
    "            \n",
    "            if i > 0:\n",
    "                key = \"{0}|{1}\".format(wordList[i], wordList[i-1])\n",
    "                if key in wordPairs:\n",
    "                    if (wordPairs.get(key)[1] != classifier):\n",
    "                        wordPairs.pop(key)\n",
    "                    else:\n",
    "                        classifier_and_count = [wordPairs.get(key)[0]+1, classifier]\n",
    "                        wordPairs[key] = classifier_and_count\n",
    "                else:\n",
    "                    classifier_and_count = [1, classifier]\n",
    "                    wordPairs[key] = classifier_and_count\n",
    "                    \n",
    "                    \n",
    "            if i < len(wordList)-2:\n",
    "                key = \"{0}|{1}|{2}\".format(wordList[i], wordList[i+1], wordList[i+2])\n",
    "                if key in wordPairs:\n",
    "                    if (wordPairs.get(key)[1] != classifier):\n",
    "                        wordPairs.pop(key)\n",
    "                    else:\n",
    "                        classifier_and_count = [wordPairs.get(key)[0]+1, classifier]\n",
    "                        wordPairs[key] = classifier_and_count\n",
    "                else:\n",
    "                    classifier_and_count = [1, classifier]\n",
    "                    wordPairs[key] = classifier_and_count\n",
    "            \n",
    "            if i > 1:\n",
    "                key = \"{0}|{1}|{2}\".format(wordList[i], wordList[i-1], wordList[i-2])\n",
    "                if key in wordPairs:\n",
    "                    if (wordPairs.get(key)[1] != classifier):\n",
    "                        wordPairs.pop(key)\n",
    "                    else:\n",
    "                        classifier_and_count = [wordPairs.get(key)[0]+1, classifier]\n",
    "                        wordPairs[key] = classifier_and_count\n",
    "                else:\n",
    "                    classifier_and_count = [1, classifier]\n",
    "                    wordPairs[key] = classifier_and_count\n",
    "    \n",
    "    # Finding the counts of all pairs of words which occur more than once and have the same output class\n",
    "    for key, value in wordPairs.items():\n",
    "        if value[0] > 1:\n",
    "            words = key.split(\"|\")\n",
    "            if len(words) == 2:\n",
    "                new_word = \"{0} {1}\".format(words[0], words[1])\n",
    "            elif len(words) == 3:\n",
    "                new_word = \"{0} {1} {2}\".format(words[0], words[1], words[2])\n",
    "            \n",
    "            for row in training_data:\n",
    "                 if new_word in row[2]:\n",
    "                    if new_word in totalWordCounts:\n",
    "                        totalWordCounts[new_word] = totalWordCounts.get(new_word) + 1\n",
    "                    else:\n",
    "                        totalWordCounts[new_word] = 1\n",
    "                    \n",
    "                    key = \"{0}|{1}\".format(new_word, row[1])\n",
    "                    if new_word in wordCounts:\n",
    "                        wordCounts[key] = wordCounts.get(key) + 1\n",
    "                    else:\n",
    "                        wordCounts[key] = 1\n",
    "            totalWords.add(new_word)\n",
    "                \n",
    "    for word in totalWordCounts:\n",
    "        for classifier in classCounts:\n",
    "            key_negative = \"P({0}=0|class={1})\".format(word, classifier)\n",
    "            key = \"{0}|{1}\".format(word, classifier)\n",
    "            \n",
    "            # Calculating probabilties \n",
    "            if key in wordCounts:\n",
    "                numerator = totalWordCounts.get(word) - wordCounts.get(key) + 1\n",
    "                denominator = totalRows - classCounts.get(classifier) + 2 \n",
    "                probability_negative = numerator / denominator\n",
    "            else:\n",
    "                probability_negative = 1\n",
    "            \n",
    "            wordProbs[key_negative] = probability_negative\n",
    "        \n",
    "    return totalRows\n",
    "        \n",
    "# Predicting outputting classes\n",
    "def predict(data):\n",
    "    counter = 0\n",
    "    for row in data:\n",
    "        counter += 1\n",
    "        text = row[1]\n",
    "        wordList = text.split(\" \")\n",
    "        word_freq = {}\n",
    "        for word in wordList:\n",
    "            if word in word_freq:\n",
    "                word_freq[word] = word_freq.get(word) + 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "        \n",
    "        probs_of_class = {}\n",
    "        for classifier in classCounts:\n",
    "            prob = classCounts.get(classifier) / totalRows\n",
    "            if prob != 0:\n",
    "                total_class_prob = m.log(prob)\n",
    "            \n",
    "            unique_wordList = list(set(wordList))\n",
    "            sum_of_probs2 = 0\n",
    "            for word in unique_wordList:\n",
    "                key = \"P({0}=0|class={1})\".format(word, classifier)\n",
    "                prob = wordProbs.get(key)\n",
    "                if word in word_freq and prob != 0 and key in wordProbs:\n",
    "                    sum_of_probs2 += m.log(prob) * word_freq.get(word)\n",
    "            \n",
    "            class_prob = total_class_prob - sum_of_probs2\n",
    "            probs_of_class[class_prob] = classifier\n",
    "        \n",
    "        prediction = probs_of_class.get(max(list(probs_of_class)))\n",
    "        temp_list = [counter, prediction]\n",
    "        predictions.append(temp_list)\n",
    "    \n",
    "totalRows = trainClassifier(training_data, totalRows)\n",
    "predict(test_data)\n",
    "\n",
    "# Uncomment to see total list of predictions\n",
    "#print(predictions)\n",
    "\n",
    "# Comment out these lines when checking kaggle accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(predictions)):\n",
    "    total += 1\n",
    "    #print(\"{0} = {1}\".format(predictions[i][1], test_values[i]))\n",
    "    if predictions[i][1] == test_values[i]:\n",
    "        correct += 1\n",
    "print(\"Accuracy: {0}\".format(correct/total))\n",
    "\n",
    "# Uncomment these two lines to generate a csv of the predictions to check kaggle accuracy\n",
    "#df = pd.DataFrame(predictions, columns=[\"id\", \"class\"])\n",
    "#df.to_csv('output.csv', index=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
