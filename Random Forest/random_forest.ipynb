{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "67209ef9-436f-4952-bb69-28c68f8e6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6197da37-ab90-4f02-97a7-88a58e07dd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5110.000000</td>\n",
       "      <td>5110.000000</td>\n",
       "      <td>5110.000000</td>\n",
       "      <td>5110.000000</td>\n",
       "      <td>5110.000000</td>\n",
       "      <td>4909.000000</td>\n",
       "      <td>5110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>36517.829354</td>\n",
       "      <td>43.226614</td>\n",
       "      <td>0.097456</td>\n",
       "      <td>0.054012</td>\n",
       "      <td>106.147677</td>\n",
       "      <td>28.893237</td>\n",
       "      <td>0.048728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21161.721625</td>\n",
       "      <td>22.612647</td>\n",
       "      <td>0.296607</td>\n",
       "      <td>0.226063</td>\n",
       "      <td>45.283560</td>\n",
       "      <td>7.854067</td>\n",
       "      <td>0.215320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17741.250000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.245000</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>36932.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.885000</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>54682.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>114.090000</td>\n",
       "      <td>33.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>72940.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>271.740000</td>\n",
       "      <td>97.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id          age  hypertension  heart_disease  \\\n",
       "count   5110.000000  5110.000000   5110.000000    5110.000000   \n",
       "mean   36517.829354    43.226614      0.097456       0.054012   \n",
       "std    21161.721625    22.612647      0.296607       0.226063   \n",
       "min       67.000000     0.080000      0.000000       0.000000   \n",
       "25%    17741.250000    25.000000      0.000000       0.000000   \n",
       "50%    36932.000000    45.000000      0.000000       0.000000   \n",
       "75%    54682.000000    61.000000      0.000000       0.000000   \n",
       "max    72940.000000    82.000000      1.000000       1.000000   \n",
       "\n",
       "       avg_glucose_level          bmi       stroke  \n",
       "count        5110.000000  4909.000000  5110.000000  \n",
       "mean          106.147677    28.893237     0.048728  \n",
       "std            45.283560     7.854067     0.215320  \n",
       "min            55.120000    10.300000     0.000000  \n",
       "25%            77.245000    23.500000     0.000000  \n",
       "50%            91.885000    28.100000     0.000000  \n",
       "75%           114.090000    33.100000     0.000000  \n",
       "max           271.740000    97.600000     1.000000  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroke_data = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
    "stroke_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "097a2010-6746-4360-b62f-1bf46a7aa52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3403, 1: 3403}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing.\n",
    "\n",
    "df = stroke_data.copy()\n",
    "df_data = df.drop(columns=[\"stroke\", \"id\"])\n",
    "df_target = df[\"stroke\"]\n",
    "\n",
    "# Let's first impute the missing values.\n",
    "# We have two columns with missing values:\n",
    "# The bmi column has missing values in the form of NaN values; we will replace these with the column's mean.\n",
    "# The smoking_status column has missing values in the form of \"Unknown\" strings; we will replace these with the column's mode.\n",
    "\n",
    "imp_bmi = SimpleImputer(strategy='mean')\n",
    "imp_smoking_status = SimpleImputer(missing_values=\"Unknown\", strategy='most_frequent')\n",
    "\n",
    "df_data[\"bmi\"] = imp_bmi.fit_transform(df_data[[\"bmi\"]])\n",
    "df_data[\"smoking_status\"] = imp_smoking_status.fit_transform(df_data[[\"smoking_status\"]])\n",
    "\n",
    "# Now we will use one-hot encoding to convert categorical vars into numeric vars\n",
    "encoded_features = pd.get_dummies(df_data[['gender', 'work_type', 'Residence_type', 'smoking_status', 'ever_married']], drop_first=True)\n",
    "\n",
    "# Concatenate the encoded features with the original DataFrame\n",
    "df_data = pd.concat([df_data, encoded_features], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "df_data.drop(['gender', 'work_type', 'Residence_type', 'smoking_status', 'ever_married'], axis=1, inplace=True)\n",
    "\n",
    "X_raw = df_data.values\n",
    "y_raw = df_target\n",
    "\n",
    "# Splitting the test and training data\n",
    "X_train_raw, X_test, y_train_raw, y_test = train_test_split(X_raw, y_raw, test_size=0.3, stratify=y, random_state=734981)\n",
    "\n",
    "\n",
    "# Next, let's fix the class imbalance in our dataset by oversampling the minority class.\n",
    "ros = RandomOverSampler(random_state=2873159)\n",
    "X_train, y_train = ros.fit_resample(X_train_raw, y_train_raw)\n",
    "\n",
    "# We can see that the classes are now balanced.\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c14f22",
   "metadata": {},
   "source": [
    "Random forest implementation & hyper-parameter tuning:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b33f7c6",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning using sklearn's RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cf391924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 2000, num = 15)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 200, num = 20)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 200, cv = 3, verbose=0, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5d25d336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1025, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 80, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d5602132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (base): 0.9413%\n",
      "Precision (base): 0.0588%\n",
      "Recall (base): 0.0133%\n",
      "F1 Score (base): 0.0217\n",
      "\n",
      "Accuracy (new): 0.9106%\n",
      "Precision (tuned): 0.122%\n",
      "Recall (new): 0.1333%\n",
      "F1 Score (new): 0.1274\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Create base (untuned)rf model\n",
    "rf_base = RandomForestClassifier()  \n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "predictions_base = rf_base.predict(X_test)\n",
    "\n",
    "# Compute accuracy & F1 score on base model\n",
    "accuracy_base = accuracy_score(y_test, predictions_base)\n",
    "f1_base = f1_score(y_test, predictions_base)\n",
    "precision_base = precision_score(y_test, predictions_base)\n",
    "recall_base = recall_score(y_test, predictions_base)\n",
    "print(f\"Accuracy (base): {round(accuracy_base, 4)}%\")\n",
    "print(f\"Precision (base): {round(precision_base, 4)}%\")\n",
    "print(f\"Recall (base): {round(recall_base, 4)}%\")\n",
    "print(f\"F1 Score (base): {round(f1_base, 4)}\")\n",
    "print()\n",
    "\n",
    "# Compute accuracy & F1 score on tuned model\n",
    "rf_tuned = RandomForestClassifier(n_estimators=1025, min_samples_split=2, min_samples_leaf=5, max_features= 'sqrt', max_depth=80, bootstrap=False)  \n",
    "rf_tuned.fit(X_train, y_train)\n",
    "predictions_tuned = rf_tuned.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, predictions_tuned)\n",
    "f1_tuned = f1_score(y_test, predictions_tuned)\n",
    "precision_tuned = precision_score(y_test, predictions_tuned)\n",
    "recall_tuned = recall_score(y_test, predictions_tuned)\n",
    "print(f\"Accuracy (new): {round(accuracy_tuned, 4)}%\")\n",
    "print(f\"Precision (tuned): {round(precision_tuned, 4)}%\")\n",
    "print(f\"Recall (new): {round(recall_tuned, 4)}%\")\n",
    "print(f\"F1 Score (new): {round(f1_tuned, 4)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9793fb04",
   "metadata": {},
   "source": [
    "The accuracy had slightly reduced in the new model, but the F1 score has improved, which is good because we want to maximise the recall and precision on the positive class. Notice that we tweaked <i>min_samples_leaf=5</i> manually so that the depth of each tree is restricted, which reduces overfitting and happens to signficantly increase the recall of our model. Recall is important because we prioritise correctly classifying examples from the positive (stroke) class (because false negatives are more dangerous than false positives in the given domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fddad8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('age', 0.41057665332887977), ('avg_glucose_level', 0.19538128779604252), ('bmi', 0.17244253629827658), ('ever_married_Yes', 0.040372291983265), ('hypertension', 0.032161433084419204), ('heart_disease', 0.028873945436842255), ('gender_Male', 0.02104858890638547), ('work_type_children', 0.018769887353160205), ('work_type_Private', 0.018201969528491777), ('Residence_type_Urban', 0.017984730119045847), ('smoking_status_never smoked', 0.017911171216334613), ('work_type_Self-employed', 0.013253505797743382), ('smoking_status_smokes', 0.01283659741378613), ('work_type_Never_worked', 0.00018540173732720425), ('gender_Other', 0.0)]\n",
      "[('age', 0.41057665332887977), ('avg_glucose_level', 0.19538128779604252), ('bmi', 0.17244253629827658)]\n"
     ]
    }
   ],
   "source": [
    "most_important_features = rf_tuned.feature_importances_\n",
    "\n",
    "feature_importance_map = {}\n",
    "for feature_index in range(0, len(most_important_features)):\n",
    "    feature_name = df_data.columns[feature_index]\n",
    "    feature_importance_map[feature_name] = most_important_features[feature_index]\n",
    "\n",
    "features_sorted_by_importance = sorted(feature_importance_map.items(), key=lambda x: x[1], reverse=True)\n",
    "print(features_sorted_by_importance)\n",
    "top_three_important_features = features_sorted_by_importance[:3]\n",
    "print(top_three_important_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
